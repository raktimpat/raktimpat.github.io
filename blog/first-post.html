<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My First AI Blog Post</title>
    <link rel="stylesheet" href="../static/css/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="/static/css/style.css">
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header class="navbar-container">
    <nav class="navbar">
        <a href="/" class="nav-logo">
            <img src="/static/images/rp_logo.png" alt="RP Logo">
        </a>
        <ul class="nav-menu">
            <li><a href="/#about">About Me</a></li>
            <li><a href="/projects.html">Projects</a></li>
            <li><a href="/blog.html">Blog</a></li>
        </ul>

        <div class="nav-right">
            <a href="https://github.com/raktimpat" target="_blank" class="nav-icon" aria-label="GitHub Profile">
                <i class="fab fa-github"></i>
            </a>
            <a href="https://www.linkedin.com/in/raktim-patar73/" target="_blank" class="nav-icon" aria-label="LinkedIn Profile">
                <i class="fab fa-linkedin"></i>
            </a>
            <a href="/#contact" class="btn btn-contact">Contact</a>
        </div>
    </nav>
</header>
    <div class="container">
        <a href="/" class="back-link">&larr; Back to Home</a>
        <article>
            <header class="post-header">
                <h1>My First AI Blog Post</h1>
                <p class="post-meta">October 01, 2025</p>
            </header>
            <div class="post-content">
                <h2>The Rise of the Transformer</h2>
<p>The world of AI was forever changed with the 2017 paper "Attention Is All You Need". This paper introduced the <strong>Transformer architecture</strong>, which has become the foundation for most state-of-the-art NLP models, including BERT and GPT.</p>
<h3>Key Components</h3>
<ol>
<li><strong>Self-Attention</strong>: Allows the model to weigh the importance of different words in the input sequence.</li>
<li><strong>Positional Encodings</strong>: Since the model doesn't process words sequentially, this injects information about the word's position.</li>
</ol>
<p>Here is a small Python code snippet:</p>
<p>```python
def self_attention(query, key, value):
    # Simplified attention mechanism
    scores = torch.matmul(query, key.transpose(-2, -1))
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, value)
    return output</p>
            </div>
        </article>
    </div>
    <div class="container">
    <hr class="footer-divider">
</div>

<footer class="site-footer">
    <div class="container">
        <p>&copy; 2025 by Raktim Patar. All rights reserved.</p>
        <div class="footer-links">
            <a href="https://github.com/raktimpat" target="_blank">GitHub</a> &bull;
            <a href="https://www.linkedin.com/in/raktim-patar73/" target="_blank">LinkedIn</a>
        </div>
    </div>
</footer>
    <script src="/static/js/main.js"></script>
</body>
</html>