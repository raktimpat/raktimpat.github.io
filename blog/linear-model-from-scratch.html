<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Build a Linear Regression Model from Scratch in Python</title>
    <link rel="stylesheet" href="../static/css/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="/static/css/style.css">
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header class="navbar-container">
    <nav class="navbar">
        <a href="/" class="nav-logo">
            <img src="/static/images/rp_logo.png" alt="RP Logo">
        </a>
        <ul class="nav-menu">
            <li><a href="/#about">About Me</a></li>
            <li><a href="/projects.html">Projects</a></li>
            <li><a href="/blog.html">Blog</a></li>
        </ul>

        <div class="nav-right">
            <a href="https://github.com/raktimpat" target="_blank" class="nav-icon" aria-label="GitHub Profile">
                <i class="fab fa-github"></i>
            </a>
            <a href="https://www.linkedin.com/in/raktim-patar73/" target="_blank" class="nav-icon" aria-label="LinkedIn Profile">
                <i class="fab fa-linkedin"></i>
            </a>
            <a href="/#contact" class="btn btn-contact">Contact</a>
        </div>
    </nav>
</header>
    <div class="container">
        <a href="/" class="back-link">&larr; Back to Home</a>
        <article>
            <header class="post-header">
                <h1>How to Build a Linear Regression Model from Scratch in Python</h1>
                <p class="post-meta">June 10, 2025</p>
            </header>
            <div class="post-content">
                <h1>How to Build a Linear Regression Model from Scratch in Python</h1>
<p>Ever wondered what's really going on inside a machine learning model? This post is for you! üßê It‚Äôs time to ditch the fancy libraries like Scikit-learn and build a simple linear regression model completely from scratch. This guide gets us to roll up our sleeves and dive into the core concepts‚Äîlike the cost function and the awesome Gradient Descent algorithm‚Äîto see how a model <em>actually</em> learns from data. By the end, you'll have a much deeper understanding of the magic behind the curtain and will have coded your very own predictive model. üöÄ</p>
<h2>About the Dataset: BMI vs. Disease Progression</h2>
<p>For this project, we'll be using the classic diabetes dataset from the scikit-learn library. This dataset contains data from 442 diabetes patients, with ten baseline variables like age, sex, body mass index (BMI), and blood pressure. The goal is to predict disease progression one year after the initial measurements.</p>
<p>For this tutorial, we will focus on the relationship between <strong>BMI</strong> and <strong>disease progression</strong>. It‚Äôs a straightforward, two-column problem perfect for understanding the core mechanics of linear regression.</p>
<h3>Why this dataset?</h3>
<p>The goal here is to learn how to <strong>build</strong> a linear regression model, not how to clean and analyze a complex dataset. We're using this specific one because:</p>
<ul>
<li><strong>It's Simple:</strong> With just one input feature (BMI) and one output (disease progression), we can focus entirely on the algorithm's code without getting distracted by complex data manipulation.</li>
<li><strong>It's Intuitive:</strong> There's a clear, positive linear relationship. As BMI increases, the disease progression tends to increase. This makes it easy to visualize and check if our model is learning correctly.</li>
<li><strong>It's a Real-World Dataset:</strong> While we are simplifying it for this tutorial, it's a real-world dataset, which gives you a taste of what it's like to work with actual health data.</li>
</ul>
<h3>A Note on Skipping EDA</h3>
<p>In a real-world project, <strong>Exploratory Data Analysis (EDA)</strong> is a crucial first step. You would normally spend significant time visualizing data, checking for missing values, and understanding feature distributions.</p>
<p>However, for this tutorial, we're making a deliberate exception and <strong>skipping a deep EDA phase</strong>. Since our primary objective is to understand the math and code behind Gradient Descent and the regression algorithm itself, we've chosen a "textbook-perfect" relationship from the dataset where the path forward is clear. This lets us jump straight into the fun part: building the model!</p>
<p>You can easily load this dataset using the <code>sklearn.datasets.load_diabetes</code> function. For more details, see the official documentation.
<strong>Documentation:</strong> <a href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset">Scikit-learn Diabetes Dataset</a></p>
<h2>The Theory: How to Teach a Computer to Draw a Line</h2>
<p>Alright, we've got the data. If you were to plot it, you‚Äôd see a cloud of dots. Our goal is to teach the computer how to draw a single straight line that slices right through the middle of that cloud, representing the data as accurately as possible.</p>
<p>This "line of best fit" is our actual <strong>model</strong>. But how do we find it? It all boils down to three core ideas.</p>
<h3>1. The Hypothesis: Just a Fancy Name for a Line</h3>
<p>Remember this from school?</p>
<blockquote>
<div class="arithmatex">\[
y = mx + c
\]</div>
</blockquote>
<p>This is the equation for a straight line. In our case, <code>y</code> is the <strong>Disease Progression</strong>, and <code>x</code> is the <strong>BMI</strong>.</p>
<blockquote>
<div class="arithmatex">\[
\text{Disease Progression} = (w \cdot \text{BMI}) + c
\]</div>
</blockquote>
<p>In machine learning, <strong>m</strong> (the slope) is called the <strong>weight</strong> (<span class="arithmatex">\(w\)</span>), and <strong>c</strong> (the y-intercept) is called the <strong>bias</strong>. These two values, <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(c\)</span>, are the only things we need to define our line. Think of them as tuning knobs. Our mission is to find the <em>perfect</em> values for these knobs so the line fits our data snugly. This equation is our <strong>hypothesis</strong>‚Äîit's our best guess about the relationship between BMI and Disease Progression.</p>
<h3>2. The Cost Function: Scoring How "Wrong" We Are</h3>
<p>So, we start by picking random values for <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(c\)</span>. The line will probably look terrible. How do we measure <em>how</em> terrible it is? We need a score.</p>
<p>For every single data point, we can measure the vertical distance between that point and our line. This distance is the <strong>error</strong>. To get a total score, we can't just add up the errors (they'd cancel each other out). So, we <strong>square</strong> each error and then calculate the average.</p>
<p>This score is called the <strong>Mean Squared Error (MSE)</strong>, and it‚Äôs our <strong>cost function</strong>. Its job is simple: to tell us, with a single number, how bad our <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(c\)</span> values are.</p>
<p><strong>The goal is to get this MSE score as low as possible.</strong></p>
<h3>3. The Mechanics of Learning: From Foggy Hillsides to Mathematical Gradients</h3>
<p>So, our model has a loss function (like MSE) that tells it how wrong it is. But how does it actually <em>learn</em> the best parameters‚Äîthe weight (<span class="arithmatex">\(w\)</span>) and bias (<span class="arithmatex">\(b\)</span>)‚Äîto minimize this error? It uses a brilliant and fundamental algorithm called <strong>Gradient Descent</strong>.</p>
<p>Imagine you're standing on a foggy hillside, and your goal is to get to the lowest point in the valley. You can't see the bottom, but you can feel the slope of the ground right where you're standing. The most logical thing to do is to take a step in the steepest <em>downhill</em> direction. You repeat this process: check the slope, take a careful step, check the slope again, and take another step. Eventually, you'll arrive at the bottom of the valley.</p>
<p>That's exactly what Gradient Descent does!</p>
<ul>
<li><strong>The Valley</strong> is our Loss Function (MSE). The lower you are, the smaller the error.</li>
<li><strong>Your Position</strong> represents the current values of your parameters, <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(b\)</span>.</li>
<li><strong>The Direction Downhill</strong> is the <strong>gradient</strong> of the loss function. A bit of calculus tells the algorithm the steepest direction to move in to reduce the error.</li>
<li><strong>The Size of Your Step</strong> is the <strong>learning rate (<span class="arithmatex">\(\eta\)</span>)</strong>. This is a small number you set. If it's too big, you might step right over the bottom of the valley! If it's too small, it will take forever to get there.</li>
</ul>
<p>This intuitive process has a precise mathematical foundation. Let's break down the core components and the calculus that powers it.</p>
<h3>The Mathematical Toolkit</h3>
<p>For our explanation, let's use our simple linear regression model. Its prediction for a single data point (<span class="arithmatex">\(x_i\)</span>) is given by:</p>
<div class="arithmatex">\[ \hat{y}_i = w x_i + b \]</div>
<p>To train this model, we need four key components:</p>
<ol>
<li><strong>Parameters:</strong> The values the model learns: the <strong>weight (<span class="arithmatex">\(w\)</span>)</strong> and the <strong>bias (<span class="arithmatex">\(b\)</span>)</strong>.</li>
<li><strong>Loss Function (<span class="arithmatex">\(L\)</span>):</strong> Our measure of total error. We'll continue with <strong>Mean Squared Error (MSE)</strong>.
    <span class="arithmatex">\(<span class="arithmatex">\(L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2\)</span>\)</span></li>
<li><strong>Learning Rate (<span class="arithmatex">\(\eta\)</span>):</strong> A small positive number (e.g., 0.01) that controls the size of our "step" during each update.</li>
<li><strong>Gradient:</strong> The derivative of the loss function. It tells us the direction of steepest <em>ascent</em> (uphill).</li>
</ol>
<h3>The General Update Rule</h3>
<p>Since the gradient points uphill, we move in the <strong>opposite</strong> direction to descend into the valley. For each training iteration, we update every parameter in the model using this simple rule:</p>
<blockquote>
<p>New Parameter = Old Parameter - (Learning Rate √ó Gradient of the Parameter)</p>
</blockquote>
<p>Mathematically, this looks like:
* <strong>Weight Update:</strong> <span class="arithmatex">\(w_{new} = w_{old} - \eta \frac{\partial L}{\partial w}\)</span>
* <strong>Bias Update:</strong> <span class="arithmatex">\(b_{new} = b_{old} - \eta \frac{\partial L}{\partial b}\)</span></p>
<p>The magic is in calculating the gradient terms: <span class="arithmatex">\(\frac{\partial L}{\partial w}\)</span> and <span class="arithmatex">\(\frac{\partial L}{\partial b}\)</span>. These are the partial derivatives that tell us the "slope" of the loss function with respect to each parameter.</p>
<h3>Calculating the "Steepest Downhill" Direction</h3>
<p>Let's calculate these gradients for our MSE loss function using the chain rule from calculus.</p>
<h4>1. Gradient of Loss with respect to Weight (<span class="arithmatex">\(w\)</span>)</h4>
<p>We need to find how the loss <span class="arithmatex">\(L\)</span> changes as we change the weight <span class="arithmatex">\(w\)</span>.</p>
<div class="arithmatex">\[ \frac{\partial L}{\partial w} = \frac{\partial}{\partial w} \left[ \frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 \right] \]</div>
<p>Applying the chain rule:
$$ \frac{\partial L}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} 2(y_i - (wx_i + b)) \cdot \frac{\partial}{\partial w}(y_i - wx_i - b) $$
$$ \frac{\partial L}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} 2(y_i - \hat{y}_i) \cdot (-x_i) $$</p>
<p>This gives us the final gradient for the weight:
$$ \frac{\partial L}{\partial w} = -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - \hat{y}_i) $$</p>
<h4>2. Gradient of Loss with respect to Bias (<span class="arithmatex">\(b\)</span>)</h4>
<p>Similarly, we find how the loss <span class="arithmatex">\(L\)</span> changes as we change the bias <span class="arithmatex">\(b\)</span>.</p>
<div class="arithmatex">\[ \frac{\partial L}{\partial b} = \frac{\partial}{\partial b} \left[ \frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2 \right] \]</div>
<p>Applying the chain rule:
$$ \frac{\partial L}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} 2(y_i - (wx_i + b)) \cdot \frac{\partial}{\partial b}(y_i - wx_i - b) $$
$$ \frac{\partial L}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} 2(y_i - \hat{y}_i) \cdot (-1) $$</p>
<p>This gives us the final gradient for the bias:
$$ \frac{\partial L}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i) $$</p>
<h3>The Learning Loop in Action</h3>
<p>Now we can substitute these specific gradients back into our general update rules:</p>
<ul>
<li><strong>Weight Update:</strong> <span class="arithmatex">\(w_{new} = w_{old} - \eta \left( -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - \hat{y}_i) \right)\)</span></li>
<li><strong>Bias Update:</strong> <span class="arithmatex">\(b_{new} = b_{old} - \eta \left( -\frac{2}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i) \right)\)</span></li>
</ul>
<p>This is the core of how the model learns. In each step, the algorithm calculates the error <span class="arithmatex">\((y_i - \hat{y}_i)\)</span> for all data points, computes the gradients, and nudges the weights and bias in the exact direction that will make the total error smaller. This process is repeated thousands of times until the loss is minimized and the model's predictions are as accurate as possible‚Äîfinally reaching the bottom of the valley.</p>
<h2>Let's Get Coding!</h2>
<p>Time to translate the theory into Python.</p>
<h3>Step 1: Import Libraries and Load Data</h3>
<p>First things first, let's import the necessary libraries‚Äî<code>numpy</code> for the math, <code>pandas</code> to load our data, and <code>matplotlib</code> to see what's going on. Then, load the dataset into a pandas DataFrame.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_diabetes</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Load the dataset</span>
<span class="n">diabetes_data</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">X_bmi</span> <span class="o">=</span> <span class="n">diabetes_data</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="c1"># Create a DataFrame for easier viewing</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_bmi</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;BMI&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">diabetes_data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;progression&#39;</span><span class="p">)</span>
</code></pre></div>

<figure style="width: 80%; max-width: 500px; margin: auto; text-align: center;">
  <img src="../static/images/blog/linear_model_from_scratch/dataframe_head.png" alt="dataframe preview">
  <figcaption>Fig. 1: Dataframe preview</figcaption>
</figure>

<h3>Step 2: Prepare and Visualize the Data</h3>
<p>Next, we'll use or extracted X and Y values from the DataFrame to plot the raw data as a scatter plot </p>
<div class="codehilite"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;BMI&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Progression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;BMI vs Disease Progression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<figure style="width: 80%; max-width: 500px; margin: auto; text-align: center;">
  <img src="../static/images/blog/linear_model_from_scratch/bmi_progression_scatter_plot.png" alt="scatter plot x and y">
  <figcaption>Fig. 2: BMI vs Disease Progression plot</figcaption>
</figure>

<h3>3. Building the Model: A Step-by-Step Code Implementation</h3>
<p>Now, let's translate our <code>LinearRegression</code> class into Python code. We'll walk through each method, explaining how it connects back to the theory we've just covered.</p>
<p>First, we define our class, <code>LinearRegression</code>. All the logic for our model will live inside this class.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LinearRegression</span><span class="p">()</span> 
</code></pre></div>

<h3>1. The <code>__init__</code> Method: Initializing the Model</h3>
<p>Every class needs an initializer. This method sets up the model's hyperparameters and prepares its internal state.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">=</span> <span class="n">iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">error_list</span><span class="o">=</span><span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>

<p><strong>Explanation:</strong></p>
<ul>
<li><code>__init__</code> is called whenever we create a new instance of our model (e.g., <code>model = LinearRegression()</code>).</li>
<li><code>learning_rate</code> (<span class="arithmatex">\(\eta\)</span>): This is the "step size" for our Gradient Descent algorithm. We give it a default value of <code>0.01</code>.</li>
<li><code>iterations</code>: This is the number of times we will run our training loop. More iterations mean more steps taken towards the minimum of the cost function.</li>
<li><code>error_list</code>, <code>weight_list</code>, <code>bias_list</code>: We create these empty lists to track how our model's error, weight, and bias change during training. This is incredibly useful for visualizing the learning process later on.</li>
<li><code>self.W</code> and <code>self.b</code>: We initialize the weight and bias to <code>None</code>. They have no value yet because the model hasn't been trained. They will get their initial values inside the <code>fit</code> method.</li>
</ul>
<h3>2. The <code>fit</code> Method: The Training Engine</h3>
<p>This is the most important method. It takes the training data (<code>X</code> and <code>Y</code>) and runs the entire Gradient Descent process.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="c1"># no_of_training_examples, no_of_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># weight initialization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
    <span class="c1"># gradient descent learning</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iterations</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>
      <span class="n">Y_pred</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span>
      <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">Y_pred</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">error_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>

<p><strong>Explanation:</strong></p>
<ol>
<li><strong>Get Dimensions:</strong> We get the number of data points (<code>m</code>) and the number of input features (<code>d</code>) from the shape of our input data <code>X</code>.</li>
<li><strong>Initialize Parameters:</strong> We set the starting point for our "walk down the foggy hillside." The weights <code>self.W</code> are initialized to a numpy array of zeros, and the bias <code>self.b</code> is set to <code>0</code>.</li>
<li><strong>Store Data:</strong> We store <code>X</code> and <code>Y</code> as instance variables so they can be easily accessed by other methods like <code>update_weights</code>.</li>
<li><strong>The Learning Loop:</strong> The <code>for</code> loop runs for the specified number of <code>iterations</code>. In each iteration, it performs one step of Gradient Descent by calling <code>self.update_weights()</code>.</li>
<li><strong>Logging Progress:</strong> Every 1000 iterations, we calculate the Mean Squared Error (MSE) and store it, along with the current weight and bias. This gives us snapshots of the learning process.</li>
</ol>
<h3>3. The <code>update_weights</code> Method: Taking a Step</h3>
<p>This method implements the core mathematical step of Gradient Descent. It calculates the gradients and updates the parameters accordingly.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="p">)</span>
    <span class="c1"># calculate gradients</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_pred</span><span class="p">))</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span>
    <span class="n">db</span> <span class="o">=</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_pred</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span>
    <span class="c1"># update weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>

<p><strong>Explanation:</strong></p>
<p>This is the math from our theory section, translated into code!</p>
<ol>
<li><strong>Get Predictions:</strong> First, we calculate the model's current predictions (<code>Y_pred</code>) with the existing <code>W</code> and <code>b</code>.</li>
<li><strong>Calculate Gradients:</strong><ul>
<li><code>dW</code>: This line calculates the gradient of the cost function with respect to the weight <code>W</code>. It's the vectorized version of the formula: <span class="arithmatex">\(\frac{\partial L}{\partial w} = -\frac{2}{n} \sum x_i (y_i - \hat{y}_i)\)</span>. <code>(self.X.T).dot(self.Y - Y_pred)</code> is a fast, <code>numpy</code>-powered way to perform the sum of <code>x * error</code>.</li>
<li><code>db</code>: This calculates the gradient with respect to the bias <code>b</code>, which is the vectorized version of: <span class="arithmatex">\(\frac{\partial L}{\partial b} = -\frac{2}{n} \sum (y_i - \hat{y}_i)\)</span>.</li>
</ul>
</li>
<li><strong>Update Parameters:</strong> We apply the update rule: <code>New Parameter = Old Parameter - (Learning Rate √ó Gradient)</code>. This nudges <code>self.W</code> and <code>self.b</code> in the direction that minimizes the error, effectively taking one step downhill.</li>
</ol>
<h3>4. The <code>predict</code> Method: Making a Guess</h3>
<p>Once the model is trained, we need a way to use it. The <code>predict</code> method takes new data and uses the learned parameters to make a prediction.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span>
</code></pre></div>

<p><strong>Explanation:</strong></p>
<ul>
<li>This is our hypothesis function: <span class="arithmatex">\(\hat{y} = wx + b\)</span>.</li>
<li><code>X.dot(self.W)</code> is the <code>numpy</code> way of calculating the dot product between the input features <code>X</code> and the learned weights <code>W</code>. This is mathematically equivalent to <span class="arithmatex">\((w \cdot x)\)</span> for all data points at once. We then add the bias <code>b</code>.</li>
</ul>
<h2>Putting It All Together: Training, Visualization, and Final Thoughts</h2>
<p>We have built the theory and the code; now let's see our model in action. The final step is to train our <code>LinearRegression</code> class on the diabetes dataset and visualize the results to truly understand the learning process.</p>
<h3>Training and Visualizing the Model</h3>
<p>The following code block will instantiate our model, train it using the <code>.fit()</code> method, and then generate a plot to show how the model's regression line improves over time.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># --- 1. Training the Model ---</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">7000</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># --- 2. Visualizing the Learning Process ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Generate and plot regression lines from our training history</span>
<span class="n">legend_labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">error_list</span><span class="p">)):</span>
  <span class="n">temp_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">weight_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">lr</span><span class="o">.</span><span class="n">bias_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="n">random_color</span> <span class="o">=</span> <span class="s2">&quot;#&quot;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;0123456789ABCDEF&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">temp_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">random_color</span><span class="p">)</span>
  <span class="n">legend_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">i</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s1">&lt;5</span><span class="si">}</span><span class="s1"> | Loss: </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">error_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># --- 3. Formatting the Plot ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;BMI&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Diabetes Progression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;BMI vs Diabetes Progression with Regression Lines at Different Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">get_lines</span><span class="p">()[</span><span class="mi">1</span><span class="p">:],</span>
           <span class="n">labels</span><span class="o">=</span><span class="n">legend_labels</span><span class="p">,</span>
           <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">,</span> <span class="n">fancybox</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Training Progress&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<figure style="margin: auto; text-align: center;">
  <img src="../static/images/blog/linear_model_from_scratch/bmi_progression_regression_lines.png" alt="scatter plot x and y" class="img-large" style="max-width:100%;">
  <figcaption>Fig. 3: Plot showing regression lines at different epochs</figcaption>
</figure>

<p>This visualization is a powerful illustration of Gradient Descent. The legend clearly shows how the model's error (Loss) decreases as the line progressively moves to find the best possible fit.</p>
<h1>For Comparison: The Scikit-learn Way</h1>
<p>In a real-world project, you would use a highly optimized library like scikit-learn. The same task can be accomplished in just a few lines:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
</code></pre></div>

<h2>1. Create and train the model</h2>
<div class="codehilite"><pre><span></span><code><span class="n">sklearn_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">sklearn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>

<h2>2. Check the results</h2>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scikit-learn weight: </span><span class="si">{</span><span class="n">sklearn_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scikit-learn bias: </span><span class="si">{</span><span class="n">sklearn_model</span><span class="o">.</span><span class="n">intercept_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Building our model from scratch was about understanding the mechanics behind the <code>.fit()</code> method. Scikit-learn provides a fast and reliable tool, but now you know the fundamental principles of how it works.</p>
<h1>Key Takeaways</h1>
<ul>
<li>
<p><strong>Linear Regression</strong> finds the best-fitting straight line (<span class="arithmatex">\(y = wx + b\)</span>) to describe the relationship between variables.</p>
</li>
<li>
<p>The "best fit" is found by minimizing a <strong>Cost Function</strong>, like Mean Squared Error (MSE), which measures the model's total error.</p>
</li>
<li>
<p><strong>Gradient Descent</strong> is the optimization algorithm that iteratively adjusts the weight (w) and bias (b) to minimize this cost, effectively "learning" from the data.</p>
</li>
<li>
<p><strong>The Learning Rate</strong> is a critical hyperparameter that controls the step size of Gradient Descent.</p>
</li>
<li>
<p>Building a model from scratch demystifies machine learning, connecting the underlying math to practical code.</p>
</li>
</ul>
<h2>End Note</h2>
<p>And there you have it! You've officially built a linear regression model from scratch. Give yourself a pat on the back‚Äîyou've turned complex math into simple, working code. The next time you use a library and type <code>.fit()</code>, you'll know the magic happening under the hood.</p>
<p>This is a huge step in your machine learning journey. The best way to learn is by doing, so keep experimenting with this code, try it on new data, and see what you can build next.</p>
<p>Keep up the great work, and happy coding! üöÄ‚ú®</p>
            </div>
        </article>
    </div>
    <div class="container">
    <hr class="footer-divider">
</div>

<footer class="site-footer">
    <div class="container">
        <p>&copy; 2025 by Raktim Patar. All rights reserved.</p>
        <div class="footer-links">
            <a href="https://github.com/raktimpat" target="_blank">GitHub</a> &bull;
            <a href="https://www.linkedin.com/in/raktim-patar73/" target="_blank">LinkedIn</a>
        </div>
    </div>
</footer>
    <script src="/static/js/main.js"></script>
</body>
</html>